{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc9b3c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "d:\\Projects\\RLHF\\rlhf\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\binit\\.cache\\huggingface\\hub\\models--distilbert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Loss: 0.7265\n",
      "Epoch 2 | Loss: 0.6789\n",
      "Epoch 3 | Loss: 0.6368\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch\n",
    "from src.reward_model import RewardModel, PreferenceDataset, preference_loss\n",
    "from torch.utils.data import DataLoader\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# Example synthetic dataset\n",
    "data = [\n",
    "    {\"prompt\": \"Why did the website crash?\",\n",
    "     \"winning\": \"Too many users logging in caused a memory overflow.\",\n",
    "     \"losing\": \"Aliens disabled the server.\"},\n",
    "    {\"prompt\": \"Why did the train stop?\",\n",
    "     \"winning\": \"Signal failure on the main line.\",\n",
    "     \"losing\": \"Because the sun forgot to rise.\"}\n",
    "]\n",
    "\n",
    "dataset = PreferenceDataset(tokenizer, data)\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = RewardModel().to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "\n",
    "# Training step\n",
    "for epoch in range(3):\n",
    "    for batch in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        winning_scores = model(batch[\"winning_input_ids\"].to(device),\n",
    "                              batch[\"winning_mask\"].to(device))\n",
    "\n",
    "        losing_scores = model(batch[\"losing_input_ids\"].to(device),\n",
    "                                batch[\"losing_mask\"].to(device))\n",
    "\n",
    "        loss = preference_loss(winning_scores, losing_scores)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1} | Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9c56bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.2411, -0.1654], grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = model()\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "775d703b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Projects\\RLHF\\rlhf\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "d:\\Projects\\RLHF\\rlhf\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\binit\\.cache\\huggingface\\hub\\datasets--nvidia--HelpSteer2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Generating train split: 100%|██████████| 20324/20324 [00:01<00:00, 10336.42 examples/s]\n",
      "Generating validation split: 100%|██████████| 1038/1038 [00:00<00:00, 5203.86 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"nvidia/HelpSteer2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10c0b80d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': \"I would like you to audit content for me based on a URL I specify below. I would like you to audit content as if you were a Google Quality Rater following the rules set out by Google (which you can see here (https://developers.google.com/search/blog/2022/08/helpful-content-update)in respect of August 2022 helpful content update (experience, expertise, authority and trust) - I would also like you to consider YMYL (your money your life where applicable) and Google medic factors also depending on the content type and nature. I would like you to provide a content quality rating based on a scale of 1 to 10 where 10 is best and 0 is worst. You should take into consideration - how well the content is written, how well it aligns with Google's August 2022 helpful content update guidelines for human quality raters, how well structured the content is, if it makes it clear what is on offer, is it gramatically correct and well written and does it fit the end users intent when comparing the main H1 tag to the body of the content. You should provide clear, actionable recommendations for any areas where the content has an issue as well as guidance to bolster expertise and trust where applicable. You should not self reference and should avoid making any assumptions, the content for you to audit can be found here: \\n\\n\\n\\n\\nhttps://redblink.com/top-ai-content-detector-tools/\",\n",
       " 'response': \"Sure, I can help you with that. I'll need access to the URL you specified, so please provide it to me.\",\n",
       " 'helpfulness': 0,\n",
       " 'correctness': 2,\n",
       " 'coherence': 3,\n",
       " 'complexity': 0,\n",
       " 'verbosity': 0}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[\"train\"][90]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e0e277",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rlhf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
